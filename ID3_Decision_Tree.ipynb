{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pyitlib import discrete_random_variable as drv\n",
    "from numpy import log2 as log\n",
    "from Evaluation import Eval\n",
    "from functions import Discretize\n",
    "from KMeans import numericCol\n",
    "eps = np.finfo(float).eps\n",
    "\n",
    "\n",
    "# def ig(e_dataset, e_attr):\n",
    "#    return (e_dataset - e_attr)\n",
    "\n",
    "\n",
    "def find_entropy(df):\n",
    "    \"\"\"\n",
    "\n",
    "    @param df: dataFrame obj\n",
    "    @return: entropy value of df\n",
    "\n",
    "    \"\"\"\n",
    "    Class = df.keys()[-1]  # To make the code generic, changing target variable class name\n",
    "    entropy = 0\n",
    "    values = df[Class].unique()\n",
    "    for value in values:\n",
    "        fraction = df[Class].value_counts()[value] / len(df[Class])\n",
    "        entropy += -fraction * np.log2(fraction)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def find_entropy_attribute(df, attribute):\n",
    "    \"\"\"\n",
    "\n",
    "    @param df: dataFrame obj\n",
    "    @param attribute: string of specific attribute\n",
    "    @return: entropy of attribute\n",
    "    \"\"\"\n",
    "    Class = df.keys()[-1]  # To make the code generic, changing target variable class name\n",
    "    target_variables = df[Class].unique()  # This gives all 'Yes' and 'No'\n",
    "    variables = df[attribute].unique()  # This gives different features in that attribute\n",
    "    entropy2 = 0\n",
    "    for variable in variables:\n",
    "        entropy = 0\n",
    "        for target_variable in target_variables:\n",
    "            num = len(df[attribute][df[attribute] == variable][df[Class] == target_variable])\n",
    "            den = len(df[attribute][df[attribute] == variable])\n",
    "            fraction = num / (den + eps)\n",
    "            entropy += -fraction * log(fraction + eps)\n",
    "            fraction2 = den / len(df)\n",
    "            entropy2 += -fraction2 * entropy\n",
    "    return abs(entropy2)\n",
    "\n",
    "\n",
    "def find_winner(df):\n",
    "    \"\"\"\n",
    "\n",
    "    @param df: dataFrame obj\n",
    "    @return: the max value of my entropy verification\n",
    "\n",
    "    \"\"\"\n",
    "    Entropy_att = []\n",
    "    IG = []\n",
    "    for key in df.keys()[:-1]:\n",
    "        # Entropy_att.append(find_entropy_attribute(df,key))\n",
    "        IG.append(find_entropy(df) - find_entropy_attribute(df, key))\n",
    "    return df.keys()[:-1][np.argmax(IG)]\n",
    "\n",
    "\n",
    "def bestIGattr(data, attributes, toSplit=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param data:\n",
    "    :param attributes:\n",
    "    :param toSplit:\n",
    "    :return: best choice by gain\n",
    "\n",
    "    \"\"\"\n",
    "    classEntropy = drv.entropy(data['class']).item(0)\n",
    "    attrsIG = {}\n",
    "    for attr in attributes:\n",
    "        attrsIG[attr] = find_entropy(data) - find_entropy_attribute(data, attr)\n",
    "    maxGain = max(attrsIG.values())\n",
    "    for attr in attrsIG:\n",
    "        if attrsIG[attr] == maxGain:\n",
    "            return attr\n",
    "\n",
    "\n",
    "def Build_Dict(data,numOfBins):\n",
    "    \"\"\"\n",
    "\n",
    "    :param data:\n",
    "    :return: attributes tree\n",
    "    \"\"\"\n",
    "    attributes = {}\n",
    "    for i in data:\n",
    "        attr = i.split()[1]\n",
    "        x = i.split()[2]\n",
    "        if i.split()[2] == 'NUMERIC':\n",
    "            field = list(range(numOfBins))\n",
    "        else:\n",
    "            field = x.replace('{', '').replace('}', '').split(',')\n",
    "        attributes[attr] = field\n",
    "    return attributes\n",
    "\n",
    "\n",
    "def buildTree(classDict, data, attributes, attrList, toSplit=False, numNodes=100):\n",
    "    \"\"\"\n",
    "    :param classDict:\n",
    "    :param data:\n",
    "    :param attributes:\n",
    "    :param attrList:\n",
    "    :param toSplit:\n",
    "    :param numNodes:\n",
    "    :return: the model,tree as a dict\n",
    "    \"\"\"\n",
    "    if len(data['class']) <= numNodes and len(data['class']) > 0:\n",
    "        return data['class'].mode().iloc[0]\n",
    "    else:\n",
    "        if len(attrList) > 0:\n",
    "            bestOp = bestIGattr(data, attrList, toSplit)\n",
    "            classDict[bestOp] = {}\n",
    "            for val in attributes[bestOp]:\n",
    "                if len(data.loc[data[bestOp] == val]) > 0 and len(attrList) > 0:\n",
    "                    newAttrsList = attrList.copy()\n",
    "                    newAttrsList.remove(bestOp)\n",
    "                    classDict[bestOp][val] = buildTree({}, data.loc[data[bestOp] == val], attributes, newAttrsList)\n",
    "            return classDict\n",
    "        else:\n",
    "            return data['class'].mode().iloc[0]\n",
    "\n",
    "\n",
    "def fun(tree, test):\n",
    "    \"\"\"\n",
    "    tree -- decision tree dictionary\n",
    "    test -- testing examples in form of pandas dataframe\n",
    "    \"\"\"\n",
    "    res = []\n",
    "\n",
    "    for _, e in test.iterrows():\n",
    "        x = predict(tree, e)\n",
    "        res.append(x)  # tree->dictionary , e -> subFrame\n",
    "    return res  # array with expected class values\n",
    "\n",
    "\n",
    "def predict(tree, subFrame):\n",
    "    \"\"\"\n",
    "    tree -- decision tree dictionary\n",
    "    subFrame -- a testing example in form of pandas series\n",
    "    \"\"\"\n",
    "    c = tree\n",
    "    while isinstance(c, dict):\n",
    "        root = list(c.keys())[0]\n",
    "        try:\n",
    "            v = subFrame[root]\n",
    "        except:\n",
    "            for i in c[root]:\n",
    "                print('r:', root)\n",
    "                print('sb|:', subFrame[root])\n",
    "                if subFrame[root] in i:\n",
    "                    v = i\n",
    "                    break\n",
    "            print('i', i)\n",
    "            v = i\n",
    "        try:\n",
    "            c = c[root][v]\n",
    "        except:\n",
    "            c = c[root][list(c[root].keys())[0]]\n",
    "    return c\n",
    "\n",
    "\n",
    "def result(arrayExpected, arrayTest):\n",
    "    \"\"\"\n",
    "    test the model against the given test file\n",
    "    :param arrayExpected:\n",
    "    :param arrayTest:\n",
    "    \"\"\"\n",
    "    match_yes = 0;\n",
    "    match_no = 0;\n",
    "    fail_no = 0;\n",
    "    fail_yes = 0;\n",
    "    for _ in range(len(arrayExpected)):\n",
    "        if arrayExpected[_] != None and arrayTest[_] != None:\n",
    "            if arrayExpected[_] == arrayTest[_]:\n",
    "                if arrayExpected[_] == 'yes':\n",
    "                    match_yes += 1\n",
    "                else:\n",
    "                    match_no += 1\n",
    "            else:\n",
    "                if arrayExpected[_] == 'yes':\n",
    "                    fail_yes += 1\n",
    "                else:\n",
    "                    fail_no += 1\n",
    "    # print('Matched values:', match)\n",
    "    # print('NON-Matched:', fail)\n",
    "    # print('ID3 Accuracy:', (match / (match + fail)), '%')\n",
    "    Eval(match_yes, match_no, fail_yes, fail_no)\n",
    "\n",
    "\n",
    "def ID3_algorithm(test, train, structFile):\n",
    "    \"\"\"\n",
    "    main program\n",
    "    :param train:\n",
    "    :param test:\n",
    "    :param structFile:\n",
    "    \"\"\"\n",
    "    numOfBins=len(train[numericCol(train,structFile)[0]].unique())\n",
    "    attributes = Build_Dict(open(structFile),numOfBins)\n",
    "    attrList = list(attributes.keys())\n",
    "    attrList.remove('class')\n",
    "    Decision_tree = buildTree({}, train, attributes, attrList)\n",
    "\n",
    "    # save model to file\n",
    "    filename = 'ID3_model.sav'\n",
    "    joblib.dump(Decision_tree, filename)\n",
    "\n",
    "    result(fun(Decision_tree, test), list(test['class']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
